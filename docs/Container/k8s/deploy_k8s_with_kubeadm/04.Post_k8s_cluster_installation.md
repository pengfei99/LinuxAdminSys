# Post k8s installation config

After k8s installation, you need to install other tools such as:
  - helm
  - reverse proxy

> I consider `cni plugin (e.g. calico, flannel, etc.)` is a part of basic component of k8s cluster.

## 1. Install helm

The official release page of helm can be found [here](https://github.com/helm/helm/releases)

You can use the below bash script to install the latest version.

```shell
vim install_helm.bash

# put the below content and run it
bash install_helm.bash

#!/bin/bash

set -euo pipefail

HELM_VERSION="v3.18.4"
HELM_TAR="helm-${HELM_VERSION}-linux-amd64.tar.gz"
HELM_URL="https://get.helm.sh/${HELM_TAR}"
TMP_DIR="/tmp/helm-install"
HELM_DIR="linux-amd64"

mkdir -p "$TMP_DIR"
cd "$TMP_DIR"

echo "Downloading Helm ${HELM_VERSION} to ${TMP_DIR}..."
wget -q "${HELM_URL}"

echo "Extracting Helm..."
tar -xzf "${HELM_TAR}"

echo "Setting execution permission..."
chmod a+x "${HELM_DIR}/helm"

echo "Moving Helm binary to /usr/local/bin..."
sudo mv "${HELM_DIR}/helm" /usr/local/bin/helm

echo "Cleaning up..."
rm -rf "$TMP_DIR"

echo "Verifying Helm installation..."
if command -v helm >/dev/null 2>&1; then
    helm version
    echo "Helm installed successfully."
else
    echo "Helm installation failed."
    exit 1
fi
```
> You should see the helm version and the success message.
> 
## 2. Set up a reverse proxy

A reverse proxy is essential for a k8s cluster. Otherwise, the app deployed in the cluster are not accessible from 
outside world. There are many possible reverse proxy solutions such as:
- Kong (commercial alternative): https://konghq.com/
- Traefik (commercial alternative): https://traefik.io/


In this tutorial, we choose **ingress-nginx**.

## The ingress nginx controller mode

There are three modes to set up the proxy and reverse proxy for a k8s cluster:
- host
- load balancer
- nodePort

### Host mode

In host mode, the `Ingress controller` it uses the host's network namespace. This means that the Ingress 
controller binds directly to the host's network interfaces and ports. 

The advantage of the host mode is that it can achieve higher performance compared to other modes, 
as it eliminates the overhead of the kube-proxy layer.

The disadvantage is that you cannot run multiple instances of the Ingress controller on the same host with the 
same ports, as there would be port conflicts.

To view the detailed configuration of host mode, check the section of `hostNetwork: true` section in the `values.yaml` 
template.



### Load balancer mode

In the load balancer mode, the Ingress controller typically runs as a service, and an `external load balancer` (normally
provided by the cloud provider) is provisioned to distribute incoming traffic to the Ingress controller service.

This mode is suitable for cloud environments where a load balancer service can be provisioned dynamically (e.g., AWS ELB, GCP Load Balancer).
The external load balancer takes care of distributing traffic to the nodes running the Ingress controller service.

To view the detailed configuration, check the section of `appProtocol:True` section

### nodePort mode

In NodePort mode, the Ingress controller service is exposed on a static port on each node in the cluster. 
This port is accessible from outside the cluster, and the traffic is then forwarded to the Ingress controller.

This mode is often used in on-premises or bare-metal environments where cloud load balancers are not available or 
in **development/testing** scenarios.

While it provides external access, it might not be as suitable for production environments due to potential 
challenges in scaling and managing external access.

### Our selected mode is Host mode

In this tutorial, we choose the **host** mode. So the `ingress-nginx` listens to the network interface of the host server.
As result, only one ingress nginx pod can be deployed on each node. And we don't want to have too many pod. So 
we added `node selector` on the ingress nginx controller service.

#### Select which node to deploy the ingress nginx controller

As we added `node selector` on the ingress nginx controller service, we also need to label one or m
ore `worker node (host server)`, otherwise no pods will be deployed.

**In our case, we label only one node**. Because we need to set up a dns resolver entry so the query can be redirected to
the node which contains the ingress controller. Even we have two pods of Ingress controller, the other one which not
in the DNS will never be used (wast of resource). 


```shell
# FQDN for k8s Ingress controller
10.50.5.68   *.casd.local
```


Below command show you how to label a node with a specific label

```shell
# to label a node, 
kubectl label node <nodename> <label-key>=<label-value>

# example
kubectl label node worker2 public=true

# to un-label a node, you can use below command
kubectl label node <nodename> <labelname>-

# example
kubectl label node worker2 public-

# after labeling, you will see new pod of nginx gets created.
kubectl get all -n ingress-nginx -w
kubectl get pods -n ingress-nginx -w

```

## Deploy the ingress nginx controller service

You can use below commands to deploy the ingress nginx controller service

```shell
# add ingress-nginx helm repo
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx

# update the repo content
helm repo update

# list available release 
helm search repo

# we want it to run is the namespace ingress-nginx, so we create a namespace
kubectl create namespace ingress-nginx

```

### Configure the ingress-nginx controller

With the above ingress-nginx repo, we can install an `ingress-nginx controller` service in out cluster.

You can find the full doc https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/

You can find the **values.yaml** template here https://github.com/kubernetes/ingress-nginx/blob/main/charts/ingress-nginx/values.yaml


#### A minimum config example

Note the below **ingress_values.yaml** is an example of how our cluster configure the ingress-nginx controller. 


```yaml
controller:
  watchIngressWithoutClass: true
  allowSnippetAnnotations: false
  # for installing nginx on the specific worker
  # all workers that have label public:"true" will have a replicas of the nginx 
  nodeSelector:
    public: "true"
  config:
    error-log-level: "info"
    ignore-invalid-headers: "false"
    proxy-request-buffering: "off"
    proxy-body-size: "0"
    large-client-header-buffers: "4 16k"

  hostNetwork: true
  extraEnvs:
    - name: MY_POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP
  kind: DaemonSet
  service:
    enabled: true
    type: ClusterIP
  ingressClassResource:
    name: nginx
    enabled: true
    default: true
    controllerValue: "k8s.io/ingress-nginx"

rbac:
  create: true
podSecurityPolicy:
  enabled: false

```

With the above configuration, you can have a minimum running ingress controller

```shell
# we deploy the ingress service with above
helm install ingress-nginx ingress-nginx/ingress-nginx -f ingress_values.yaml -n ingress-nginx 

# 
kubectl get all -n ingress-nginx
```

After the pod of ingress are created, you can try to send request to the worker where nginx is deployed.

```shell
# If the ip address of the worker is 10.0.2.8, you can try below command
curl 10.0.2.8

# if you see below output, it means ingress-nginx is running and answering request
<html>
<head><title>404 Not Found</title></head>
<body>
<center><h1>404 Not Found</h1></center>
<hr><center>nginx</center>
</body>
</html>
```

You can try to deploy the mario app, and check the certificate. You will notice, ingress will assign a `fake certificat`.

We need to replace this certificate by our valid certificate.

### Add a default certificate for all services

Create a secret to host the certificate and private key. In this tutorial, we name the secret as 
`casd-wildcard-certificate`, you can use the below command

```shell

# general form
kubectl create secret tls <secret-name> --namespace <namespace-name> --key=pathTo/ingress-tls.key --cert=pathTo/ingress-tls.crt -o yaml

# example
kubectl create secret tls casd-wildcard-certificate --key=wildcard-casd.key --cert=wildcard-casd.crt -o yaml -n ingress-nginx 

# view the content of the secret, the certificate and private value is in base64, you need to decode it to view the
# value. No encryption at all, so we need to pay attention on who can view this secret.
kubectl get secret casd-wildcard-certificate -o jsonpath='{.data}' -n ingress-nginx 

# you can also edit the value directly
kubectl edit secret casd-wildcard-certificate -n ingress-nginx 
```

To tell the ingress to use the given certificate, you need to use **extraArgs.default-ssl-certificate** config. Below
is a full example. Then you need to update the ingress controller with new configuration

```yaml
controller:
  watchIngressWithoutClass: true
  allowSnippetAnnotations: false
  # for installing nginx on the specific worker
  # all workers that have label public:"true" will have a replicas of the nginx 
  nodeSelector:
    public: "true"
  config:
    error-log-level: "info"
    ignore-invalid-headers: "false"
    proxy-request-buffering: "off"
    proxy-body-size: "0"
    large-client-header-buffers: "4 16k"
  # enable host mode 
  hostNetwork: true
  extraEnvs:
    - name: MY_POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP
  kind: DaemonSet
  service:
    enabled: true
    type: ClusterIP
  ingressClassResource:
    name: nginx
    enabled: true
    default: true
    controllerValue: "k8s.io/ingress-nginx"

    # Parameters is a link to a custom resource containing additional
    # configuration for the controller. This is optional if the controller
    # does not require extra parameters.
    parameters: {}
  # add a default certificate for all ingress
  # no need to use  `- secretName: casd-test-tls-secret` in ingress.yaml
  # to specify a custom certificate
  # the default certificate should be a wildcard which covers your domain
  extraArgs:
    default-ssl-certificate: "ingress-nginx/casd-wildcard-certificate"
rbac:
  create: true
podSecurityPolicy:
  enabled: false

```


### Update existing ingress-nginx deployment

The best way to update a deployment (deployed via helm chart) is to modify the `values.yaml`. Then call the below command

```shell
# general form
helm upgrade <deployment-name> <chart-name> -f <config-file> -n <namespace>

# example
helm upgrade ingress-nginx ingress-nginx/ingress-nginx -f ingress_values.yaml -n ingress-nginx

# to delete 
helm delete ingress-nginx
```